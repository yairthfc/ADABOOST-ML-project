# âš¡ AdaBoost From Scratch â€” With Decision Stumps & Full Visualization

This repository implements the **AdaBoost ensemble algorithm from scratch**, using custom-built weak learners (Decision Stumps).  
It also includes:

- Full AdaBoost training pipeline  
- Decision surface plots  
- Weighted-sample visualization  
- Error curves for training and testing  
- Theoretical handwritten PDF  
- Synthetic dataset generator  

All code is written manually using NumPy â€” **no scikitâ€‘learn models**.

---

# ğŸ“ Project Structure

```
adaboost-from-scratch/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ adaboost.py            # AdaBoost implementation
â”‚   â”œâ”€â”€ decision_stump.py      # Weak learner used by AdaBoost
â”‚   â”œâ”€â”€ adaboost_scenario.py   # Full experiment runner + plots
â”‚   â”œâ”€â”€ loss_functions.py      # Misclassification error
â”‚   â”œâ”€â”€ utils.py               # Plotly helpers (decision surface, etc.)
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ Answers.pdf            # Theoretical derivations & handwritten solutions
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# ğŸš€ Implemented Components

## ğŸ”¹ **AdaBoost Algorithm**
Source: `src/adaboost.py`  

Implements the full AdaBoost classifier using:

- Custom weak learner `DecisionStump`
- Weighted sample distribution \( D_t \)
- Weak learner error calculation
- Learner weight (alpha) calculation  
  \[
  lpha_t = rac{1}{2}\lnrac{1-\epsilon_t}{\epsilon_t}
  \]
- Updated sample weights  
- Final prediction by weighted sign of learners  
- Partial predictions for first \( T \) learners  
- Partial losses

The class also stores all historical weight vectors \( D_t \) for visualization.

---

## ğŸ”¹ **Decision Stump (Weak Learner)**
Source: `src/decision_stump.py`  

Implements a CART-style decision stump supporting:

- Single feature split  
- Threshold search along sorted feature values  
- Best sign (+1 / âˆ’1)  
- Misclassification-based threshold search  
- Predicting based on:  
  *â€œbelow threshold â†’ âˆ’sign, above threshold â†’ +signâ€*

This stump is specifically suited for AdaBoost.

---

## ğŸ”¹ **Synthetic Dataset Generator & Experiment Runner**
Source: `src/adaboost_scenario.py`

Includes:

### âœ” Dataset generator  
Creates 2â€‘dimensional points in \([-1,1]^2\) and labels them according to a circle decision rule.  
Supports a configurable noise ratio.

### âœ” Training AdaBoost  
Trains AdaBoost with:

- Any number of learners (default 250)
- Weighted stumps
- Full train/test evaluation
- Partial losses for each \( t \)

### âœ” Visualizations  
- **Training vs Test error** over boosting iterations  
- **Decision surfaces** for:  
  - 5 learners  
  - 50 learners  
  - 100 learners  
  - 250 learners  
- **Best performing T** automatically identified  
- **Weighted-sample decision surface**  
  (sample size proportional to final weight \( D_T \))

---

# ğŸ“Š Visualization Examples

These correspond to the figures shown in the handwritten PDF (`docs/Answers.pdf`):

### 1ï¸âƒ£ Training vs Test Error Curve  
Shows that:

- Train error decreases rapidly  
- Test error stabilizes  
- Overfitting depends on noise level  

### 2ï¸âƒ£ Decision Surfaces for Different Numbers of Learners  
Shows boostingâ€™s improvement over time.

### 3ï¸âƒ£ Best Learner Count  
Automatic selection of the \( T \) minimizing test loss.

### 4ï¸âƒ£ Weighted Samples Visualization  
Plots training points with marker sizes proportional to weights.

---

# ğŸ§  Theoretical Component

Included in: `docs/Answers.pdf`  
Contains handwritten derivations:

- AdaBoost weight update proof  
- Boosting intuition  
- Error analysis  
- VC-dimension context  
- Figures with loss curves and decision boundaries  
- Analysis for noise vs no-noise cases  

---

# ğŸ“¦ Installation

```bash
pip install -r requirements.txt
```

---

# â–¶ï¸ Usage

### Run all experiments:
```bash
python src/adaboost_scenario.py
```

This will:

- Generate datasets  
- Train AdaBoost  
- Produce evaluation plots  
- Display decision surfaces  
- Highlight best-performing \( T \)  
- Show weighted sample decision surface  

---

# â–¶ï¸ Manual Usage Example

```python
from adaboost import AdaBoost
from decision_stump import DecisionStump

model = AdaBoost(wl=DecisionStump, iterations=200)
model.fit(X_train, y_train)

pred = model.predict(X_test)
loss = model.loss(X_test, y_test)
```

---

# ğŸ›  Technologies Used

- Python  
- NumPy  
- Plotly  
- Custom ML implementations  

---

# ğŸ¯ Learning Outcomes

This project demonstrates:

- Understanding of ensemble learning  
- Implementing AdaBoost from scratch  
- Weak learner vs strong learner behavior  
- Sample reweighting & exponential loss  
- Decision boundary evolution  
- Visualization of boosting dynamics  
- Bias/variance behavior under noise  

---

# ğŸ“˜ License
MIT License.

---

# ğŸ™Œ Author  
**Yair Mahfud**
